---
title: Model Development and Apparent Validation
---

```{r}
source("notebooks/initialize-data-analysis.r")
source("notebooks/BMA-model-no-initial-var.r") # run_bas_glm() function
source("notebooks/prob-cal_intercept.r") # valProbggplot() function
source("notebooks/prob-cal_slope.r")

load("data/results/variables-to-screen.rda")
load("data/results/variables-to-screen-short.rda")
load("data/results/variables-to-force.rda")
```

# Penalized Likelihood Methods
## Backwards Stepwise Logistic Regression (obsolete)
```{r}
# library(StepReg)
# run_stepwise_logit <- function(data, formula) {
#   stepwise(
#     formula,
#     data = data,
#     type = "logit",
#     include = NULL,
#     strategy = "backward",
#     metric = "AIC"
#   )
# }

# stepwise_model <- data_patient_transformed |>
#   select(all_of(variables_to_screen_short), flag_ADR_TP_ID) |>
#   run_stepwise_logit(
#     formula = flag_ADR_TP_ID ~ .
#   )

# stepwise_model
```

```{r}
# run_step_AIC <- function(data, formula) {
#   MASS::stepAIC(
#     glm(
#       formula = formula,
#       data = data
#     ),
#     direction = "backward",
#     trace = FALSE
#   )
# }

# stepwise_model_MASS <- data_patient_transformed |>
#   select(all_of(predictor_transformed), flag_ADR_TP_ID) |>
#   run_step_AIC(
#     formula = flag_ADR_TP_ID ~ .
#   )

# stepwise_model_MASS |> str()
```

## LASSO Variable Selection (obsolete, see Elastic Net Regularization)

```{r}
# library(glmnet)
# predictor_matrix <- data_patient_complete |>
#   select(all_of(predictor_list)) |>
#   as.matrix()
# response_matrix <- data_patient_complete |>
#   select(flag_ADR_TP_ID) |>
#   pull()

# lasso_fit <- glmnet(
#   x = predictor_matrix,
#   y = response_matrix,
#   family = "binomial",
#   alpha = 1
# )

# cv_lasso_fit <- cv.glmnet(
#   x = predictor_matrix,
#   y = response_matrix,
#   family = "binomial",
#   alpha = 1,
#   nfolds = 5
# )

# coef(lasso_fit, s = cv_lasso_fit$lambda.min)@i |> length() - 1
```

## Elastic Net Regularization

```{r}
library(tidymodels)
library(future)
library(tictoc)
library(splines2)

data_patient_cross_prep_factor <- data_patient_transformed |>
  select(all_of(variables_to_screen_short), flag_ADR_TP_ID, site) |>
  mutate(
    across(where(is.logical), as.factor)
  )

data_patient_screen <- data_patient_transformed |>
  select(all_of(variables_to_screen_short), flag_ADR_TP_ID) |>
  mutate(
    across(where(is.logical), as.factor)
  )

cross_splits <- group_initial_split(data_patient_cross_prep_factor, group = "site")

cross_samples <- group_vfold_cv(data_patient_cross_prep_factor, group = "site")
```

```{r}
rec <- recipe(flag_ADR_TP_ID ~ ., data = data_patient_screen) |>
  step_dummy(all_nominal_predictors())

model_elastic <- logistic_reg(penalty = tune(), mixture = tune()) |>
  set_engine("glmnet") |>
  set_mode("classification")

model_workflow <- workflow() |>
  add_model(model_elastic) |>
  add_recipe(rec)

tune_params <- model_workflow |>
  extract_parameter_set_dials() |>
  update(
    penalty = penalty(range = c(-3, 0)),
    mixture = mixture(range = c(0.5, 1))
  ) |>
  grid_latin_hypercube(size = 1000)

tic()
plan(multisession, workers = max(min(parallel::detectCores() - 1, 12), 2))

model_tune_results <- model_workflow |>
  tune_grid(
    resamples = cross_samples,
    grid = tune_params,
    control = control_grid(save_pred = TRUE)
  )

plan(sequential)
toc()

model_accepted_flow <- model_tune_results |>
  select_by_one_std_err(desc(mixture), desc(penalty), metric = "roc_auc")

model_glmnet <- model_workflow |>
  finalize_workflow(model_accepted_flow) |>
  fit(data = data_patient_screen)

model_workflow |>
  finalize_workflow(model_accepted_flow) |>
  fit_resamples(
    resamples = cross_samples,
    metrics = metric_set(roc_auc, brier_class, cal_intercept, cal_slope),
    control = control_resamples(save_pred = TRUE)
  ) |>
  collect_metrics()

model_glmnet |> tidy()
```

```{r}
# multi_glmnet <- model_glmnet |>
#   tidy() |>
#   filter(estimate > 0) |>
#   pull(term) |>
#   str_extract(paste(variables_to_screen, collapse = "|"))

# model_glmnet_fit <-
#   logistic_reg(
#     penalty = model_accepted_flow$penalty,
#     mixture = model_accepted_flow$mixture
#   ) |>
#   set_engine("glmnet") |>
#   fit(
#     flag_ADR_TP_ID ~ .,
#     data = data_patient_screen
#   ) |>
#   tidy()
```

# Bayesian Methods

## Bayesian Model Selection

### Strategy 1-A: Screen "consensus" variables, continuous variables only (13 parameters)
```{r}
# multi_model <- data_patient_complete |>
#   select(all_of(predictor_list), flag_ADR_TP_ID) |>
#   run_bas_glm(
#     formula = flag_ADR_TP_ID ~ .,
#     include.always = ~1
#   )

# summary(multi_model)
# plot(multi_model)
# image(multi_model, rotate = FALSE)
# diagnostics(multi_model)

library(tictoc)

tic()
multi_model_transformed <- data_patient_transformed |>
  select(all_of(variables_to_screen_short), flag_ADR_TP_ID) |>
  run_bas_glm(
    formula = flag_ADR_TP_ID ~ .,
    include.always = ~1,
    n.models = 2^15,
    thin = length(variables_to_screen_short),
    force.heredity = TRUE,
    update = 1
  )
toc()

summary(multi_model_transformed)
plot(multi_model_transformed)
image(multi_model_transformed, rotate = FALSE)
# diagnostics(multi_model_transformed)

multi_predict_HPM <- variable.names(predict(multi_model_transformed, estimator = "HPM"))[-1] |>
  str_extract(paste(variables_to_screen_short, collapse = "|")) |>
  unique() # highest probability model
multi_predict_BPM <- variable.names(predict(multi_model_transformed, estimator = "BPM"))[-1] |>
  str_extract(paste(variables_to_screen_short, collapse = "|")) |>
  unique() # best predictive model, this model is weird
```

```{r}
library(tidymodels)
library(probably)
library(discrim)

model_basic_flow <- workflow() |>
  add_variables(outcomes = "flag_ADR_TP_ID", predictors = all_of(multi_predict_BPM)) |>
  add_model(logistic_reg())

# model_full <- model_basic_flow |>
#   update_variables(outcomes = "flag_ADR_TP_ID", predictors = all_of(multi_predict_HPM)) |>
#   fit(data = data_patient_screen)

model_full_BPM <- model_basic_flow |>
  update_variables(outcomes = "flag_ADR_TP_ID", predictors = all_of(multi_predict_BPM)) |>
  fit(data = data_patient_screen)

# model_basic_performances <- workflow() |>
#   add_variables(outcomes = "flag_ADR_TP_ID", predictors = all_of(multi_predict_BPM)) |>
#   add_model(logistic_reg()) |>
#   fit_resamples(
#     resamples = cross_samples,
#     metrics = metric_set(roc_auc, brier_class, cal_intercept, cal_slope, sens, spec, ppv, npv),
#     control = control_resamples(save_pred = TRUE)
#   )

# model_basic_performances |>
#   collect_metrics()

# model_basic_performances |>
#   collect_predictions() |>
#   threshold_perf(flag_ADR_TP_ID, .pred_TRUE, thresholds = seq(0.1, 0.9, by = 0.1))

save(model_full_BPM, file = "data/model-performance/model-full-BPM-1-A.rda")
```

### Strategy 1-B: Force top 5, screen "consensus", continuous only

### Strategy 1-C: Force top 5 (allow categorical), screen "consensus", continuous only

### Strategy 1-D: Force top 3, screen "consensus", continuous only

### Strategy 2-A: Screen top 21 (by mean), continuous only

### Strategy 2-B: Force top 5, screen top 21 (by mean), continuous only

### Strategy 2-C: Force top 5 (allow categorical), screen top 21 (by mean), continuous only

### Strategy 2-D: Force top 3, screen top 21 (by mean), continuous only

### Strategy 3-A: Screen all, continuous only

### Strategy 3-B: Force top 5, screen all, continuous only

### Strategy 3-C: Force top 5 (allow categorical), screen all, continuous only

### Strategy 3-D: Force top 3, screen all, continuous only

## Model Performance (completed in notebook 04, this will be removed)
```{r}
# library(CalibrationCurves)
# pHat <- predict(model_full_BPM$fit$fit$fit, data_patient_screen, type = "response")
# yTest <- data_patient_screen$flag_ADR_TP_ID |> as.logical()
# calperf <- valProbggplot(pHat, yTest)

# apparent_performance_metrics <- tibble(
#   cross_C_index = calperf$Cindex[[1]],
#   cross_C_index_lower = calperf$Cindex[[2]],
#   cross_C_index_upper = calperf$Cindex[[3]],
#   cross_calibration_intercept = calperf$Calibration$Intercept[[1]],
#   cross_calibration_intercept_lower = calperf$Calibration$Intercept[[2]],
#   cross_calibration_intercept_upper = calperf$Calibration$Intercept[[3]],
#   cross_calibration_slope = calperf$Calibration$Slope[[1]],
#   cross_calibration_slope_lower = calperf$Calibration$Slope[[2]],
#   cross_calibration_slope_upper = calperf$Calibration$Slope[[3]]
# ) |>
#   mutate(
#     cross_C_index_std_error = (cross_C_index_upper - cross_C_index_lower) / (2 * 1.96),
#     cross_calibration_intercept_std_error = (cross_calibration_intercept_upper - cross_calibration_intercept_lower) / (2 * 1.96),
#     cross_calibration_slope_std_error = (cross_calibration_slope_upper - cross_calibration_slope_lower) / (2 * 1.96)
#   ) |>
#   select(
#     cross_calibration_intercept, cross_calibration_intercept_std_error, cross_calibration_slope, cross_calibration_slope_std_error
#   ) |>
#   rename(
#     cal_in_the_large = cross_calibration_intercept, 
#     cal_in_the_large_std_error = cross_calibration_intercept_std_error, 
#     cal_slope = cross_calibration_slope, 
#     cal_slope_std_error = cross_calibration_slope_std_error) |>
#   pivot_longer(
#     cols = everything(),
#     names_to = c(".metric"),
#     values_to = c("mean")
#   ) |>
#   mutate(std_err = if_else(str_detect(.metric, "std_error"), mean, lead(mean))) |>
#   filter(!str_detect(.metric, "std_error")) |>
#   bind_rows(model_basic_performances |> collect_metrics())

# save(apparent_performance_metrics, file = "data/model-performance/apparent-performance-metrics.rda")
```

```{r}
# pHat <- predict(model_full_BPM$fit, data_patient_transformed, type = "response")
# yTest <- data_patient_transformed$flag_ADR_TP_ID
# calperf <- valProbggplot(pHat, yTest)

# apparent_performance_metrics_BPM <- tibble(
#   apparent_C_index = calperf$Cindex[[1]],
#   apparent_calibration_intercept = calperf$Calibration$Intercept[[1]],
#   apparent_calibration_slope = calperf$Calibration$Slope[[1]]
# )

# save(apparent_performance_metrics_BPM, file = "data/model-performance/apparent-performance-metrics-BPM.rda")
```

## Bayesian Generalized Additive Model (WIP)

```{r}
# library(spikeSlabGAM)
# run_spike_slab_GAM <- function(data, formula, ...) {
#   spikeSlabGAM(formula, data = data, family = "binomial", ...)
# }

# data_model <- data_patient_transformed |>
#   select(all_of(variables_to_screen), flag_ADR_TP_ID) |>
#   select(-c("baseline_CLCR_30", "baseline_CLCR_60", "LZD_duration_10", "baseline_PLT_150", "patient_age_65")) |>
#   as.data.frame()

# options(mc.cores = max(min(parallel::detectCores() - 1, 12), 2))

# model_0 <-
#   run_spike_slab_GAM(
#     data = data_model,
#     formula = reformulate(
#       termlabels = colnames(data_model) |> setdiff(c("flag_ADR_TP_ID")),
#       response = "flag_ADR_TP_ID"
#     )
#   )

# summary(model_0)
# plot(model_0)
```

## Nomogram with rms (WIP)

```{r}
# library(splines2)

# library(rms)

# data_patient_transformed |>
#   select(all_of(predictor_list), flag_ADR_TP_ID) |>
#   select_if(is.numeric) |>
#   map(quantile, probs = c(0, 0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99, 1))

# dd <- data_patient_transformed |>
#   select(all_of(predictor_list), flag_ADR_TP_ID) |>
#   datadist()

# dd$limits$patient_age <- c(50, 62, 73, 19, 91, 19, 92)
# dd$limits$baseline_PLT <- c(143, 206, 288, 21, 433, 18, 434)
# dd$limits$baseline_CLCR <- c(21, 48, 84, 5, 184, 5, 189)
# dd$limits$LZD_duration <- c(6, 9, 14, 3, 26, 3, 28)

# options(datadist = "dd")
# nomo_obj <- lrm(flag_ADR_TP_ID ~ rcs(patient_age, 3) + rcs(baseline_PLT, 3) + rcs(baseline_CLCR, 3) + rcs(LZD_duration, 3) + invasive_CRRT + comed_heparin, data = data_patient_transformed) |>
#   nomogram(fun = plogis, funlabel = "Risk") |>
#   plot()

# for(k in 3:5) {
#   f <- lrm(flag_ADR_TP_ID ~ rcs(patient_age, k) + rcs(baseline_PLT, k) + rcs(baseline_CLCR, k) + rcs(LZD_duration, k) + invasive_CRRT * comed_heparin, data = data_patient_transformed)
#   cat('k=', k, '  AIC=', AIC(f), '\n')
# }

# print.nomogram <- function(x, dec = 0, ...) {
#   obj <- x
#   w <- diff(range(obj$lp$x)) / diff(range(obj$lp$x.real))
#   cat(
#     "Points per unit of linear predictor:", format(w),
#     "\nLinear predictor units per point   :", format(1 / w), "\n\n"
#   )

#   fun <- FALSE
#   for (x in names(obj)) {
#     k <- x == "total.points" || x == "lp" || x == "abbrev"
#     if (k) {
#       fun <- TRUE
#       next
#     }
#     y <- obj[[x]]
#     if (fun) {
#       z <- cbind(round(y[[1]], dec), y$x.real)
#       dimnames(z) <- list(rep("", nrow(z)), c("Total Points", x))
#     } else {
#       z <- cbind(format(y[[1]]), format(round(y$points, dec)))
#       dimnames(z) <- list(rep("", length(y$points)), c(x, "Points"))
#       ## didn't use data.frame since wanted blank row names
#     }
#     cat("\n")
#     print(z, quote = FALSE)
#     cat("\n")
#   }
#   invisible()
# }
```

# Check Model Assumptions

## Linearity

Visual check of linearity assumption using partial-residual plots (component+residual plots).

Blue dashed line: linear fit of the partial residuals.

Red line: smoothed conditional mean

The lines should be close together and the red line should be linear. 

```{r}
# car::crPlot(model_full$fit, variable = "patient_age")
# car::crPlot(model_full$fit, variable = "baseline_PLT")

car::crPlot(model_full_BPM |> extract_fit_engine(), variable = "patient_age")
car::crPlot(model_full_BPM |> extract_fit_engine(), variable = "baseline_PLT")
car::crPlot(model_full_BPM |> extract_fit_engine(), variable = "baseline_CLCR")
```

Result: No obvious non-linearity in the partial-residual plots.

Visual check of linearity assumption using logit of predicted probabilities vs. continuous predictors values.
```{r}
# data_patient_linear_check <- data_patient_transformed |>
#   select(all_of(multi_predict_HPM)) |>
#   mutate(
#     probabilities = predict(model_full$fit, data_patient_transformed, type = "response"),
#     log_odds = log(probabilities / (1 - probabilities))
#   ) |>
#   select_if(is.numeric) |>
#   gather(key = "predictors", value = "predictor_value", -log_odds, -probabilities)

# ggplot(data_patient_linear_check, aes(y = log_odds, x = predictor_value)) +
#   geom_point(size = 0.5, alpha = 0.5) +
#   geom_smooth(method = "loess", color = "red") +
#   geom_smooth(method = "lm", linetype = "dashed", color = "blue", se = FALSE) +
#   theme_bw() +
#   facet_wrap(~predictors, scales = "free_x") +
#   labs(title = "Highest Probability Model", x = "Predictor Value", y = "Log Odds")

data_patient_linear_check_BPM <- data_patient_transformed |>
  select(all_of(multi_predict_BPM)) |>
  mutate(
    probabilities = predict(model_full_BPM |> extract_fit_engine(), data_patient_screen, type = "response"),
    log_odds = log(probabilities / (1 - probabilities))
  ) |>
  select_if(is.numeric) |>
  gather(key = "predictors", value = "predictor_value", -log_odds, -probabilities)

ggplot(data_patient_linear_check_BPM, aes(y = log_odds, x = predictor_value)) +
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess", color = "red") +
  geom_smooth(method = "lm", linetype = "dashed", color = "blue", se = FALSE) +
  theme_bw() +
  facet_wrap(~predictors, scales = "free_x") +
  labs(title = "Best Predictive Model", x = "Predictor Value", y = "Log Odds")
```

## No Multicollinearity

VIFs from the coefficient estimates should be less than 5 and as close to 1 as possible.

```{r}
car::vif(model_full_BPM |> extract_fit_engine())
```

Result: VIFs are all less than 5 and close to 1.

## No Outlier Effects

Visual check from influence plot.

Individual observations should not have too much influence on the model.

Influence plot: 

- x-axis: Extreme values (Hat-values)
- y-axis: Leverage (Studentized residuals)
- Size of points: Cook's distance

```{r}
# car::influencePlot(model_full$fit)

car::influencePlot(model_full_BPM |> extract_fit_engine())

influence.measures(model_full_BPM |> extract_fit_engine())$is.inf |>
  apply(1, any) |>
  which() |>
  unname()
```

```{r}
# data_patient_transformed |>
#   slice(c(292, 301, 335, 364, 504, 634)) |> summary()
```

```{r}
# save(model_full, file = "data/model-performance/model-full.rda")
save(model_full_BPM, file = "data/model-performance/model-full-BPM.rda")
save(model_glmnet, file = "data/model-performance/model-glmnet.rda")
```