---
title: Model Development and Apparent Validation
---

```{r}
source("notebooks/initialize-data-analysis.r")
source("notebooks/BMA-model-no-initial-var.r") # run_bas_glm() function
```

# Penalized Likelihood Methods
## Backwards Stepwise Logistic Regression
```{r}
# library(StepReg)
# run_stepwise_logit <- function(data, formula) {
#   stepwiseLogit(
#     formula,
#     data = data,
#     include = NULL,
#     selection = "backward",
#     select = "AIC"
#   )
# }

# stepwise_model <- data_patient_complete |>
#   select(all_of(predictor_list), flag_ADR_TP_ID) |>
#   run_stepwise_logit(
#     formula = flag_ADR_TP_ID ~ .
#   )

# stepwise_model |> summary()
```

```{r}
run_step_AIC <- function(data, formula) {
  MASS::stepAIC(
    glm(
      formula = formula,
      data = data
    ),
    direction = "backward",
    trace = FALSE
  )
}

stepwise_model <- data_patient_complete |>
  select(all_of(predictor_list), flag_ADR_TP_ID) |>
  run_step_AIC(
    formula = flag_ADR_TP_ID ~ .
  )

stepwise_model$coefficients |> length()
```

Results: Stepwise algorithm selected 17 predictors.

## LASSO Variable Selection

```{r}
library(glmnet)
predictor_matrix <- data_patient_complete |>
  select(all_of(predictor_list)) |>
  as.matrix()
response_matrix <- data_patient_complete |>
  select(flag_ADR_TP_ID) |>
  pull()

lasso_fit <- glmnet(
  x = predictor_matrix,
  y = response_matrix,
  family = "binomial",
  alpha = 1
)

cv_lasso_fit <- cv.glmnet(
  x = predictor_matrix,
  y = response_matrix,
  family = "binomial",
  alpha = 1,
  nfolds = 5
)

coef(lasso_fit, s = cv_lasso_fit$lambda.min)@i |> length() - 1
```

Results: LASSO algorithm selected 22-25 predictors.

Comment: Penalized likelihood methods have comparable predictive performance to Bayesian methods [citation needed], but select many more predictors.

# Bayesian Model Selection
```{r}
# multi_model <- data_patient_complete |>
#   select(all_of(predictor_list), flag_ADR_TP_ID) |>
#   run_bas_glm(
#     formula = flag_ADR_TP_ID ~ .,
#     include.always = ~1
#   )

# summary(multi_model)
# plot(multi_model)
# image(multi_model, rotate = FALSE)
# diagnostics(multi_model)

multi_model_transformed <- data_patient_transformed |>
  select(all_of(predictor_transformed), flag_ADR_TP_ID) |>
  run_bas_glm(
    formula = flag_ADR_TP_ID ~ .,
    include.always = ~1 + comed_heparin
  )

summary(multi_model_transformed)
plot(multi_model_transformed)
image(multi_model_transformed, rotate = FALSE)
diagnostics(multi_model_transformed)

multi_predict_HPM <- variable.names(predict(multi_model_transformed, estimator = "HPM"))[-1] |>
  str_extract(paste(predictor_transformed, collapse = "|")) # highest probability model
multi_predict_BPM <- variable.names(predict(multi_model_transformed, estimator = "BPM"))[-1] |>
  str_extract(paste(predictor_transformed, collapse = "|")) # best predictive model, this model is weird
```

```{r}
library(tidymodels)

data_patient_transformed_response_as_factor <- data_patient_transformed |>
  mutate(
    flag_ADR_TP_ID = as.factor(flag_ADR_TP_ID)
  )

model_full <- logistic_reg() |>
  set_engine("glm") |>
  set_mode("classification") |>
  fit(
    reformulate(
      termlabels = multi_predict_HPM,
      response = "flag_ADR_TP_ID"
    ),
    data = data_patient_transformed_response_as_factor
  )

# dd <- data_patient_transformed_response_as_factor |>
#   select(all_of(predictor_list), flag_ADR_TP_ID) |>
#   datadist()

# options(datadist = "dd")
# lrm(flag_ADR_TP_ID ~ rcs(patient_age, 4) + rcs(baseline_PLT, 4) + rcs(baseline_CLCR, 4) + rcs(LZD_duration, 4) + invasive_CRRT + comed_heparin, data = data_patient_transformed_response_as_factor) |>
#   nomogram(fun = plogis, funlabel = "Risk") |> plot()

# print.nomogram <- function(x, dec = 0, ...) {
#   obj <- x
#   w <- diff(range(obj$lp$x)) / diff(range(obj$lp$x.real))
#   cat(
#     "Points per unit of linear predictor:", format(w),
#     "\nLinear predictor units per point   :", format(1 / w), "\n\n"
#   )

#   fun <- FALSE
#   for (x in names(obj)) {
#     k <- x == "total.points" || x == "lp" || x == "abbrev"
#     if (k) {
#       fun <- TRUE
#       next
#     }
#     y <- obj[[x]]
#     if (fun) {
#       z <- cbind(round(y[[1]], dec), y$x.real)
#       dimnames(z) <- list(rep("", nrow(z)), c("Total Points", x))
#     } else {
#       z <- cbind(format(y[[1]]), format(round(y$points, dec)))
#       dimnames(z) <- list(rep("", length(y$points)), c(x, "Points"))
#       ## didn't use data.frame since wanted blank row names
#     }
#     cat("\n")
#     print(z, quote = FALSE)
#     cat("\n")
#   }
#   invisible()
# }
```

# Check Model Assumptions

## Linearity

Visual check of linearity assumption using partial-residual plots (component+residual plots).

Blue dashed line: linear fit of the partial residuals.

Red line: smoothed conditional mean

The lines should be close together and the red line should be linear. 

```{r}
car::crPlot(model_full$fit, variable = "patient_age")
car::crPlot(model_full$fit, variable = "baseline_PLT")
```

Result: No obvious non-linearity in the partial-residual plots.

Visual check of linearity assumption using logit of predicted probabilities vs. continuous predictors values.
```{r}
data_patient_linear_check <- data_patient_transformed |>
  select(all_of(multi_predict_HPM)) |>
  mutate(
    probabilities = predict(model_full$fit, data_patient_transformed, type = "response"),
    log_odds = log(probabilities / (1 - probabilities))
  ) |>
  select_if(is.numeric) |>
  gather(key = "predictors", value = "predictor_value", -log_odds, -probabilities)

ggplot(data_patient_linear_check, aes(y = log_odds, x = predictor_value)) +
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess", color = "red") +
  geom_smooth(method = "lm", linetype = "dashed", color = "blue", se = FALSE) +
  theme_bw() +
  facet_wrap(~predictors, scales = "free_x")
```

## No Multicollinearity

VIFs from the coefficient estimates should be less than 5 and as close to 1 as possible.

```{r}
car::vif(model_full$fit)
```

Result: VIFs are all less than 5 and close to 1.

## No Outlier Effects

Visual check from influence plot.

Individual observations should not have too much influence on the model.

Influence plot: 

- x-axis: Extreme values (Hat-values)
- y-axis: Leverage (Studentized residuals)
- Size of points: Cook's distance

```{r}
car::influencePlot(model_full$fit)
# influential_obs <- influence.measures(model_full$fit)$is.inf |> apply(1, any) |> which() |> unname()
```

Result: 

```{r}
# data_patient_transformed |>
#   slice(c(292, 301, 335, 364, 504, 634)) |> summary()
```

# Model Performance
```{r}
library(CalibrationCurves)
pHat <- predict(model_full$fit, data_patient_transformed, type = "response")
yTest <- data_patient_transformed$flag_ADR_TP_ID
calperf <- valProbggplot(pHat, yTest)

apparent_performance_metrics <- tibble(
  apparent_C_index = calperf$Cindex[[1]],
  apparent_calibration_intercept = calperf$Calibration$Intercept[[1]],
  apparent_calibration_slope = calperf$Calibration$Slope[[1]]
)

save(apparent_performance_metrics, file = "data/model-performance/apparent-performance-metrics.rda")
```

```{r}
save(model_full, file = "data/model-performance/model-full.rda")
```