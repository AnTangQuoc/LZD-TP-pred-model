{
  "hash": "c2f3ec6173e023df55557e117cb8fc5d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Bootstrap Development and Validation\n---\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nsource(\"notebooks/initialize-data-analysis.r\")\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nsource(\"notebooks/BMA-model-no-initial-var.r\") # run_bas_glm() function\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(rsample) # for bootstraps()\nlibrary(furrr) # for future_map()\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nLoading required package: future\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n✔ broom        1.0.5      ✔ recipes      1.0.10\n✔ dials        1.2.1      ✔ tune         1.1.2 \n✔ infer        1.0.6      ✔ workflows    1.1.4 \n✔ modeldata    1.3.0      ✔ workflowsets 1.0.1 \n✔ parsnip      1.2.0      ✔ yardstick    1.3.0 \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(CalibrationCurves)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nLoading required package: rms\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nLoading required package: Hmisc\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\nAttaching package: 'Hmisc'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nThe following object is masked from 'package:parsnip':\n\n    translate\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nThe following objects are masked from 'package:dplyr':\n\n    src, summarize\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\ndata_patient_bootstrap_prep <- data_patient_complete |>\n  select(all_of(predictor_list), flag_ADR_TP_ID)\n\n# Create 500 bootstrap samples\nboot_samples <- bootstraps(data_patient_bootstrap_prep, times = 30)\n\nextract_variable_names <- function(sample) {\n  data <- analysis(sample)\n  model <- data |>\n    run_bas_glm(\n      formula = flag_ADR_TP_ID ~ .\n    )\n  variable.names(predict(model, estimator = \"HPM\"))[-1] |>\n    str_extract(paste(predictor_list, collapse = \"|\"))\n}\n\n# Function to modify sample data and fit model\nfit_model_to_sample <- function(sample, variables) {\n  data <- analysis(sample) |>\n    mutate(flag_ADR_TP_ID = as.factor(flag_ADR_TP_ID))\n\n  formula <- reformulate(termlabels = variables, response = \"flag_ADR_TP_ID\")\n\n  logistic_reg() |>\n    set_engine(\"glm\") |>\n    set_mode(\"classification\") |>\n    fit(formula, data = data)\n}\n\ncalc_boot_performance <- function(sample, model) {\n  data <- analysis(sample)\n  pHat <- predict(model$fit, data, type = \"response\")\n  yTest <- data$flag_ADR_TP_ID\n  calperf <- valProbggplot(pHat, yTest, smooth = \"none\")\n\n  tibble(\n    boot_C_index = calperf$Cindex[[1]],\n    boot_calibration_intercept = calperf$Calibration$Intercept[[1]],\n    boot_calibration_slope = calperf$Calibration$Slope[[1]]\n  )\n}\n\ncalc_test_performance <- function(model) {\n  pHat <- predict(model$fit, data_patient_complete, type = \"response\")\n  yTest <- data_patient_complete$flag_ADR_TP_ID\n  calperf <- valProbggplot(pHat, yTest, smooth = \"none\")\n\n  tibble(\n    test_C_index = calperf$Cindex[[1]],\n    test_calibration_intercept = calperf$Calibration$Intercept[[1]],\n    test_calibration_slope = calperf$Calibration$Slope[[1]]\n  )\n}\n\n# Set up future to use multiple cores\nplan(multisession, workers = min(parallel::detectCores() - 2, 12))\n\nboot_predict_HPM <- boot_samples$splits |>\n  future_map(extract_variable_names, .options = furrr_options(seed = TRUE))\n\n# Fit model to each sample in parallel\nboot_full <- future_map2(boot_samples$splits, boot_predict_HPM, fit_model_to_sample)\n\nboot_performance_metrics <- future_map2_dfr(boot_samples$splits, boot_full, calc_boot_performance)\n\n# Calculate performance metrics for each model in parallel\ntest_performance_metrics <- boot_full |>\n  future_map_dfr(calc_test_performance)\n\nplan(sequential)\n\n# Combine performance metrics\nperformance_metrics <- bind_cols(boot_performance_metrics, test_performance_metrics)\n\n# Calculate optimism\nperformance_metrics <- performance_metrics |>\n  mutate(\n    optimism_C_index = boot_C_index - test_C_index,\n    optimism_calibration_intercept = boot_calibration_intercept - test_calibration_intercept,\n    optimism_calibration_slope = boot_calibration_slope - test_calibration_slope\n  )\n\noptimism_estimates <- performance_metrics |>\n  summarise(\n    mean_optimism_C_index = mean(optimism_C_index),\n    mean_optimism_calibration_intercept = mean(optimism_calibration_intercept),\n    mean_optimism_calibration_slope = mean(optimism_calibration_slope)\n  )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nload(\"data/model-performance/apparent-performance-metrics.rda\")\n\ncorrected_performance_metrics <- bind_cols(apparent_performance_metrics, optimism_estimates) |>\n  mutate(\n    corrected_C_index = apparent_C_index - mean_optimism_C_index,\n    corrected_calibration_intercept = apparent_calibration_intercept - mean_optimism_calibration_intercept,\n    corrected_calibration_slope = apparent_calibration_slope - mean_optimism_calibration_slope\n  ) |>\n  select(starts_with(\"corrected\"))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nsave(corrected_performance_metrics, file = \"data/model-performance/corrected-performance-metrics.rda\")\n```\n:::",
    "supporting": [
      "04-bootstrap-performance_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}